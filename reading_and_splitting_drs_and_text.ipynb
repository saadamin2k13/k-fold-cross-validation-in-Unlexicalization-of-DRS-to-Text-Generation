{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Reading Train, Dev, and Test files for merging.**"
      ],
      "metadata": {
        "id": "h3iz04z9sq5Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading data and getting Gold-PMB files."
      ],
      "metadata": {
        "id": "ShBHbpG_B-hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf sample_data/"
      ],
      "metadata": {
        "id": "C1KmU60xsqah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://pmb.let.rug.nl/releases/exp_data_3.0.0.zip\"\n"
      ],
      "metadata": {
        "id": "UlstK_qPBM4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip exp_data_3.0.0.zip\n"
      ],
      "metadata": {
        "id": "2HL5YBVqBckZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm exp_data_3.0.0.zip\n"
      ],
      "metadata": {
        "id": "BeD94-bqBjWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv ./pmb_exp_data_3.0.0/en/gold/* ."
      ],
      "metadata": {
        "id": "4XgiJv7pBuQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf pmb_exp_data_3.0.0/"
      ],
      "metadata": {
        "id": "ul5dDg79B44D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging DRS and Text files."
      ],
      "metadata": {
        "id": "6BsIHfRLCGCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat train.txt.raw dev.txt.raw test.txt.raw > sentences.txt"
      ],
      "metadata": {
        "id": "ZLWaGxjXtGPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat train.txt dev.txt test.txt > drs.txt"
      ],
      "metadata": {
        "id": "9U_wN4y_ugTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading and Splitting Dataset for K-Fold validation. Having 80% training, 10% validation and 10% testing data division. Also saving in new files.**"
      ],
      "metadata": {
        "id": "WkfUHNMU-tOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to split train and test data."
      ],
      "metadata": {
        "id": "ANeXcCr07t23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sentences_file = 'sentences.txt'\n",
        "drs_file = 'drs.txt'\n",
        "train_sentences_file = 'train_sentences.txt'\n",
        "train_drs_file = 'train_drs.txt'\n",
        "val_sentences_file = 'test_sentences.txt'\n",
        "val_drs_file = 'test_drs.txt'\n",
        "\n",
        "# Read in the sentences\n",
        "with open(sentences_file, 'r') as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Read in the DRSs\n",
        "with open(drs_file, 'r') as f:\n",
        "    drs = f.read().split('\\n\\n')\n",
        "\n",
        "# Combine the sentences and DRSs into tuples\n",
        "data = list(zip(sentences, drs))\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
        "\n",
        "# Separate the training and validation sets back into separate lists\n",
        "train_sentences, train_drs = zip(*train_data)\n",
        "val_sentences, val_drs = zip(*val_data)\n",
        "\n",
        "# Write the training sentences and DRSs to new files\n",
        "with open(train_sentences_file, 'w') as f:\n",
        "    for sentence in train_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "with open(train_drs_file, 'w') as f:\n",
        "    for dr in train_drs:\n",
        "        f.write(dr + '\\n\\n')\n",
        "\n",
        "# Write the validation sentences and DRSs to new files\n",
        "with open(val_sentences_file, 'w') as f:\n",
        "    for sentence in val_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "with open(val_drs_file, 'w') as f:\n",
        "    for dr in val_drs:\n",
        "        f.write(dr + '\\n\\n')\n"
      ],
      "metadata": {
        "id": "FmBGQf-V-405"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv train_drs.txt drs1.txt\n",
        "!mv train_sentences.txt sentences1.txt"
      ],
      "metadata": {
        "id": "SDMMTqpj75K_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to split train and validation data."
      ],
      "metadata": {
        "id": "3BypAWgk7xm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sentences_file = 'sentences1.txt'\n",
        "drs_file = 'drs1.txt'\n",
        "train_sentences_file = 'train_sentences.txt'\n",
        "train_drs_file = 'train_drs.txt'\n",
        "val_sentences_file = 'dev_sentences.txt'\n",
        "val_drs_file = 'dev_drs.txt'\n",
        "\n",
        "# Read in the sentences\n",
        "with open(sentences_file, 'r') as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Read in the DRSs\n",
        "with open(drs_file, 'r') as f:\n",
        "    drs = f.read().split('\\n\\n')\n",
        "\n",
        "# Combine the sentences and DRSs into tuples\n",
        "data = list(zip(sentences, drs))\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
        "\n",
        "# Separate the training and validation sets back into separate lists\n",
        "train_sentences, train_drs = zip(*train_data)\n",
        "val_sentences, val_drs = zip(*val_data)\n",
        "\n",
        "# Write the training sentences and DRSs to new files\n",
        "with open(train_sentences_file, 'w') as f:\n",
        "    for sentence in train_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "with open(train_drs_file, 'w') as f:\n",
        "    for dr in train_drs:\n",
        "        f.write(dr + '\\n\\n')\n",
        "\n",
        "# Write the validation sentences and DRSs to new files\n",
        "with open(val_sentences_file, 'w') as f:\n",
        "    for sentence in val_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "with open(val_drs_file, 'w') as f:\n",
        "    for dr in val_drs:\n",
        "        f.write(dr + '\\n\\n')\n"
      ],
      "metadata": {
        "id": "JgGQOrro70nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing fold-1 dataset."
      ],
      "metadata": {
        "id": "i-3WI9bQ9Mls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm dev_*\n",
        "!rm test_*\n",
        "!rm train_*"
      ],
      "metadata": {
        "id": "bJdCKhYc9QoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm drs1.txt sentences1.txt"
      ],
      "metadata": {
        "id": "MRwwChxk9geu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Shuffling DRS and Text Dataset files for other K-Fold Experiments.**"
      ],
      "metadata": {
        "id": "MS3A1wuQ86O7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "and saving the shuffled data into new DRS and Text files."
      ],
      "metadata": {
        "id": "VOpkeXdl-l3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "sentences_file = 'sentences.txt'\n",
        "drs_file = 'drs.txt'\n",
        "shuffled_sentences_file = 'shuffled_sentences.txt'\n",
        "shuffled_drs_file = 'shuffled_drs.txt'\n",
        "\n",
        "# Read in the sentences\n",
        "with open(sentences_file, 'r') as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Read in the DRSs\n",
        "with open(drs_file, 'r') as f:\n",
        "    drs = f.read().split('\\n\\n')\n",
        "\n",
        "# Combine the sentences and DRSs into tuples\n",
        "data = list(zip(sentences, drs))\n",
        "\n",
        "# Shuffle the tuples\n",
        "random.shuffle(data)\n",
        "\n",
        "# Separate the shuffled tuples back into separate lists\n",
        "shuffled_sentences, shuffled_drs = zip(*data)\n",
        "\n",
        "# Write the shuffled sentences to a new file\n",
        "with open(shuffled_sentences_file, 'w') as f:\n",
        "    for sentence in shuffled_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "# Write the shuffled DRSs to a new file\n",
        "with open(shuffled_drs_file, 'w') as f:\n",
        "    for dr in shuffled_drs:\n",
        "        f.write(dr + '\\n\\n')\n"
      ],
      "metadata": {
        "id": "k7oBlzsz89L9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm sentences.txt drs.txt"
      ],
      "metadata": {
        "id": "TRldMAtAxwkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv shuffled_sentences.txt sentences.txt"
      ],
      "metadata": {
        "id": "-_oAUwO8x3fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv shuffled_drs.txt drs.txt"
      ],
      "metadata": {
        "id": "RH5lswAayDiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sentences_file = 'sentences.txt'\n",
        "drs_file = 'drs.txt'\n",
        "train_sentences_file = 'train_sentences2.txt'\n",
        "train_drs_file = 'train_drs2.txt'\n",
        "val_sentences_file = 'test_sentences2.txt'\n",
        "val_drs_file = 'test_drs2.txt'\n",
        "\n",
        "# Read in the sentences\n",
        "with open(sentences_file, 'r') as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Read in the DRSs\n",
        "with open(drs_file, 'r') as f:\n",
        "    drs = f.read().split('\\n\\n')\n",
        "\n",
        "# Combine the sentences and DRSs into tuples\n",
        "data = list(zip(sentences, drs))\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
        "\n",
        "# Separate the training and validation sets back into separate lists\n",
        "train_sentences, train_drs = zip(*train_data)\n",
        "val_sentences, val_drs = zip(*val_data)\n",
        "\n",
        "# Write the training sentences and DRSs to new files\n",
        "with open(train_sentences_file, 'w') as f:\n",
        "    for sentence in train_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "with open(train_drs_file, 'w') as f:\n",
        "    for dr in train_drs:\n",
        "        f.write(dr + '\\n\\n')\n",
        "\n",
        "# Write the validation sentences and DRSs to new files\n",
        "with open(val_sentences_file, 'w') as f:\n",
        "    for sentence in val_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "with open(val_drs_file, 'w') as f:\n",
        "    for dr in val_drs:\n",
        "        f.write(dr + '\\n\\n')\n"
      ],
      "metadata": {
        "id": "1BjY5yMcxoxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv train_drs2.txt drs2.txt\n",
        "!mv train_sentences2.txt sentences2.txt"
      ],
      "metadata": {
        "id": "NUrlzcGB-Cl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to split train and validation data."
      ],
      "metadata": {
        "id": "5uO87lB9-CmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sentences_file = 'sentences2.txt'\n",
        "drs_file = 'drs2.txt'\n",
        "train_sentences_file = 'train_sentences2.txt'\n",
        "train_drs_file = 'train_drs2.txt'\n",
        "val_sentences_file = 'dev_sentences2.txt'\n",
        "val_drs_file = 'dev_drs2.txt'\n",
        "\n",
        "# Read in the sentences\n",
        "with open(sentences_file, 'r') as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Read in the DRSs\n",
        "with open(drs_file, 'r') as f:\n",
        "    drs = f.read().split('\\n\\n')\n",
        "\n",
        "# Combine the sentences and DRSs into tuples\n",
        "data = list(zip(sentences, drs))\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
        "\n",
        "# Separate the training and validation sets back into separate lists\n",
        "train_sentences, train_drs = zip(*train_data)\n",
        "val_sentences, val_drs = zip(*val_data)\n",
        "\n",
        "# Write the training sentences and DRSs to new files\n",
        "with open(train_sentences_file, 'w') as f:\n",
        "    for sentence in train_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "with open(train_drs_file, 'w') as f:\n",
        "    for dr in train_drs:\n",
        "        f.write(dr + '\\n\\n')\n",
        "\n",
        "# Write the validation sentences and DRSs to new files\n",
        "with open(val_sentences_file, 'w') as f:\n",
        "    for sentence in val_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "with open(val_drs_file, 'w') as f:\n",
        "    for dr in val_drs:\n",
        "        f.write(dr + '\\n\\n')\n"
      ],
      "metadata": {
        "id": "I5pKuptk-CmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing fold-2 dataset."
      ],
      "metadata": {
        "id": "IqcN6gXM-CmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm dev_*\n",
        "!rm test_*\n",
        "!rm train_*"
      ],
      "metadata": {
        "id": "RSuuposR-CmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm drs2.txt sentences2.txt"
      ],
      "metadata": {
        "id": "UH_yb04q-CmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle again"
      ],
      "metadata": {
        "id": "IyunaFk3yvm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "sentences_file = 'sentences.txt'\n",
        "drs_file = 'drs.txt'\n",
        "shuffled_sentences_file = 'shuffled_sentences.txt'\n",
        "shuffled_drs_file = 'shuffled_drs.txt'\n",
        "\n",
        "# Read in the sentences\n",
        "with open(sentences_file, 'r') as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Read in the DRSs\n",
        "with open(drs_file, 'r') as f:\n",
        "    drs = f.read().split('\\n\\n')\n",
        "\n",
        "# Combine the sentences and DRSs into tuples\n",
        "data = list(zip(sentences, drs))\n",
        "\n",
        "# Shuffle the tuples\n",
        "random.shuffle(data)\n",
        "\n",
        "# Separate the shuffled tuples back into separate lists\n",
        "shuffled_sentences, shuffled_drs = zip(*data)\n",
        "\n",
        "# Write the shuffled sentences to a new file\n",
        "with open(shuffled_sentences_file, 'w') as f:\n",
        "    for sentence in shuffled_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "# Write the shuffled DRSs to a new file\n",
        "with open(shuffled_drs_file, 'w') as f:\n",
        "    for dr in shuffled_drs:\n",
        "        f.write(dr + '\\n\\n')\n"
      ],
      "metadata": {
        "id": "bmrYsq6gyxOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm sentences.txt drs.txt"
      ],
      "metadata": {
        "id": "mM2hOVbNzB5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv shuffled_sentences.txt sentences.txt"
      ],
      "metadata": {
        "id": "Fu_A6y1kzB5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv shuffled_drs.txt drs.txt"
      ],
      "metadata": {
        "id": "R4vPR9NVzB5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sentences_file = 'sentences.txt'\n",
        "drs_file = 'drs.txt'\n",
        "train_sentences_file = 'train_sentences3.txt'\n",
        "train_drs_file = 'train_drs3.txt'\n",
        "val_sentences_file = 'test_sentences3.txt'\n",
        "val_drs_file = 'test_drs3.txt'\n",
        "\n",
        "# Read in the sentences\n",
        "with open(sentences_file, 'r') as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Read in the DRSs\n",
        "with open(drs_file, 'r') as f:\n",
        "    drs = f.read().split('\\n\\n')\n",
        "\n",
        "# Combine the sentences and DRSs into tuples\n",
        "data = list(zip(sentences, drs))\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
        "\n",
        "# Separate the training and validation sets back into separate lists\n",
        "train_sentences, train_drs = zip(*train_data)\n",
        "val_sentences, val_drs = zip(*val_data)\n",
        "\n",
        "# Write the training sentences and DRSs to new files\n",
        "with open(train_sentences_file, 'w') as f:\n",
        "    for sentence in train_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "with open(train_drs_file, 'w') as f:\n",
        "    for dr in train_drs:\n",
        "        f.write(dr + '\\n\\n')\n",
        "\n",
        "# Write the validation sentences and DRSs to new files\n",
        "with open(val_sentences_file, 'w') as f:\n",
        "    for sentence in val_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "with open(val_drs_file, 'w') as f:\n",
        "    for dr in val_drs:\n",
        "        f.write(dr + '\\n\\n')\n"
      ],
      "metadata": {
        "id": "6NT2Ei5RzNsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv train_drs3.txt drs3.txt\n",
        "!mv train_sentences3.txt sentences3.txt"
      ],
      "metadata": {
        "id": "FdHlhXpW-9vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to split train and validation data."
      ],
      "metadata": {
        "id": "r3rCES1L-9vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sentences_file = 'sentences3.txt'\n",
        "drs_file = 'drs3.txt'\n",
        "train_sentences_file = 'train_sentences3.txt'\n",
        "train_drs_file = 'train_drs3.txt'\n",
        "val_sentences_file = 'dev_sentences3.txt'\n",
        "val_drs_file = 'dev_drs3.txt'\n",
        "\n",
        "# Read in the sentences\n",
        "with open(sentences_file, 'r') as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Read in the DRSs\n",
        "with open(drs_file, 'r') as f:\n",
        "    drs = f.read().split('\\n\\n')\n",
        "\n",
        "# Combine the sentences and DRSs into tuples\n",
        "data = list(zip(sentences, drs))\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, val_data = train_test_split(data, test_size=0.1, random_state=42)\n",
        "\n",
        "# Separate the training and validation sets back into separate lists\n",
        "train_sentences, train_drs = zip(*train_data)\n",
        "val_sentences, val_drs = zip(*val_data)\n",
        "\n",
        "# Write the training sentences and DRSs to new files\n",
        "with open(train_sentences_file, 'w') as f:\n",
        "    for sentence in train_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "with open(train_drs_file, 'w') as f:\n",
        "    for dr in train_drs:\n",
        "        f.write(dr + '\\n\\n')\n",
        "\n",
        "# Write the validation sentences and DRSs to new files\n",
        "with open(val_sentences_file, 'w') as f:\n",
        "    for sentence in val_sentences:\n",
        "        f.write(sentence + '\\n')\n",
        "\n",
        "with open(val_drs_file, 'w') as f:\n",
        "    for dr in val_drs:\n",
        "        f.write(dr + '\\n\\n')\n"
      ],
      "metadata": {
        "id": "Z_XjQH_k-9vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "removing fold-3 dataset."
      ],
      "metadata": {
        "id": "0ADAjvjh-9vS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm dev_*\n",
        "!rm test_*\n",
        "!rm train_*"
      ],
      "metadata": {
        "id": "ztDiCSoc-9vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm drs3.txt sentences3.txt"
      ],
      "metadata": {
        "id": "hLS3bhJU-9vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**finding sentence length for each Test file in k-Fold experiment.**\n",
        "this is to compare the length for each k-fold based test set."
      ],
      "metadata": {
        "id": "N-qQKlOIjhCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"test.txt\", \"r\") as input_file:\n",
        "    with open(\"t_lengths.txt\", \"w\") as output_file:\n",
        "        for line in input_file:\n",
        "            sentence = line.strip()\n",
        "            length = len(sentence)\n",
        "            output_file.write(str(length) + \"\\n\")"
      ],
      "metadata": {
        "id": "9l-NPoe6jo2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading DRS and Text Files**"
      ],
      "metadata": {
        "id": "xZZMjs5O8yz2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8Ra1i6C6g1Z"
      },
      "outputs": [],
      "source": [
        "sentences_file = 'sentences.txt'\n",
        "drs_file = 'drs.txt'\n",
        "\n",
        "# Read in the sentences\n",
        "with open(sentences_file, 'r') as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Read in the DRSs\n",
        "with open(drs_file, 'r') as f:\n",
        "    drs = f.read().split('\\n\\n')\n",
        "\n",
        "# Make sure the number of sentences and DRSs match\n",
        "assert len(sentences) == len(drs)\n",
        "\n",
        "# Print the first sentence and its corresponding DRS\n",
        "print(sentences[0])\n",
        "print(drs[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "another way of reading DRS and Text files and verifying number of lines in both dataset format i.e., DRS and Text"
      ],
      "metadata": {
        "id": "7Qv17Vuo827N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_file = 'sentences.txt'\n",
        "drs_file = 'drs.txt'\n",
        "\n",
        "\n",
        "# Read in the sentences\n",
        "with open(sentences_file, 'r') as f:\n",
        "    sentences = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# Read in the DRSs\n",
        "with open(drs_file, 'r') as f:\n",
        "    drs = f.read().split('\\n\\n')\n",
        "\n",
        "# Print the lengths of the two lists\n",
        "print(len(sentences))\n",
        "print(len(drs))\n",
        "\n",
        "# Make sure the number of sentences and DRSs match\n",
        "assert len(sentences) == len(drs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VDQQMgs7kJN",
        "outputId": "800b7368-cd84-45d2-dbce-686707ee8787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "885\n",
            "885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7YXUPqpG4br",
        "outputId": "8d128980-eef3-48c1-ceb9-bb560fb5fcf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drs.txt        shuffled_drs.txt        train_sentences.txt\n",
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/   shuffled_sentences.txt  val_drs.txt\n",
            "sentences.txt  train_drs.txt           val_sentences.txt\n"
          ]
        }
      ]
    }
  ]
}